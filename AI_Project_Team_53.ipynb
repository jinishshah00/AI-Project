{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI Project Team 53.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp4K7X6E95Bt"
      },
      "source": [
        "## **Setting up Enviroment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKJs07Rz4tsR"
      },
      "source": [
        "\n",
        "#install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PndWSQMu4xHp"
      },
      "source": [
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIyzKBE8427A"
      },
      "source": [
        "#'''\n",
        "file_loc = './sample_data/california_housing_train.csv'\n",
        "#'''"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_dJTns_4vKM"
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "# Mount Dataset from GDrive\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QSx2UG-WJ9Z"
      },
      "source": [
        "### **Import Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGQ0qvn1WPRR"
      },
      "source": [
        "import librosa \n",
        "import librosa.display\n",
        "!pip install SoundFile\n",
        "import soundfile \n",
        "import os, glob\n",
        "from time import time\n",
        "import pickle \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.neural_network import MLPClassifier \n",
        "from sklearn.svm import SVC \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import make_scorer, confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seQpTWWuoBMF"
      },
      "source": [
        "## **Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8lYq_2loIHR"
      },
      "source": [
        "def extract_feature(file_name, mfcc, chroma, mel, contrast, tonnetz):\n",
        "    with soundfile.SoundFile(file_name) as sound_file:\n",
        "        X,sr = librosa.core.load(file_name, mono=True, dtype=\"float32\")\n",
        "        #X = sound_file.read(always_2d=True, dtype=\"float32\")\n",
        "        sample_rate=sound_file.samplerate\n",
        "        #sound_file.channels = 1\n",
        "        if chroma:\n",
        "            stft=np.abs(librosa.stft(X))\n",
        "        result=np.array([])\n",
        "        if mfcc:\n",
        "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "            result=np.hstack((result, mfccs))\n",
        "        if chroma:\n",
        "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "            result=np.hstack((result, chroma))\n",
        "        if mel:\n",
        "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "            result=np.hstack((result, mel))\n",
        "        if contrast:\n",
        "            contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
        "            result = np.hstack((result, contrast))\n",
        "        if tonnetz:\n",
        "            tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
        "            result = np.hstack((result, tonnetz))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iVKGoVGorEC"
      },
      "source": [
        "#emotions as suggested in the RAVDESS dataset filenames\n",
        "emotions={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}\n",
        "\n",
        "#classes we want to classify the emotions in\n",
        "observed_emotions=['03', '04', '05', '08']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91o6-5Enr-hr"
      },
      "source": [
        "## **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gw3q0_ssELY"
      },
      "source": [
        "def load_data(test_size=0.2):\n",
        "    x,y=[],[]\n",
        "    for file in glob.glob(\"/content/gdrive/My Drive/Colab Datasets/RAVDESS_Dataset/Actor_*//*.wav\"):\n",
        "        file_name=os.path.basename(file)\n",
        "        emotion=file_name.split(\"-\")[2]\n",
        "        if emotion not in observed_emotions:\n",
        "            continue\n",
        "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True, contrast=True, tonnetz=True)\n",
        "        x.append(feature)\n",
        "        y.append(emotion)\n",
        "        #tvs = TrainValidationSplit(trainRatio=0.5)\n",
        "#    return tvs\n",
        "    return x,y\n",
        "\n",
        "\n",
        "#import IPython.display as ipd\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#ipd.Audio('/content/gdrive/My Drive/Colab Datasets/RAVDESS_Dataset/Actor_01/03-01-01-01-01-01-01.wav')\n",
        "\n",
        "#data, sampling_rate = librosa.load('/content/gdrive/My Drive/Colab Datasets/RAVDESS_Dataset/Actor_01/03-01-01-01-01-01-01.wav')\n",
        "#plt.figure(figsize=(12, 4))\n",
        "#librosa.display.waveplot(data, sr=sampling_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4lLj625wFvF"
      },
      "source": [
        "## **Split Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK8C8AK61ECF"
      },
      "source": [
        "x,y=load_data(test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68EmMXX_znUS"
      },
      "source": [
        "z = train_test_split(np.array(x), y, test_size=0.25, random_state=9)\n",
        "x_train, x_test, y_train, y_test = z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klWPVqrt7xZj",
        "outputId": "58ef7a4b-aadc-4458-af6b-8f2d17f1b99f"
      },
      "source": [
        "print(\"No. of training samples:\", x_train.shape[0],\" No. of testing samples:\", x_test.shape[0])\n",
        "#print(type(x_train))\n",
        "x=np.array(x)\n",
        "print(x.shape)\n",
        "#print(type(x))\n",
        "#print(type(y))\n",
        "#print(type(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of training samples: 504  No. of testing samples: 168\n",
            "(672, 193)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzvNPV8JwOIZ"
      },
      "source": [
        "## **Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJGCnWYuPa00",
        "outputId": "a2271ce5-785b-4f54-9297-503f5529be10"
      },
      "source": [
        "print(f'No. of Features extracted: {x_train.shape[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of Features extracted: 193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYbTpmXCoFXt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "857458f3-d35b-42a0-b013-bfebacf082a7"
      },
      "source": [
        "'''\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Colab Datasets/RAVDESS_Dataset/Actor_*//*.wav\"):\n",
        "  wav_file = AudioSegment.from_file(file = file, format=\"wav\")\n",
        "  #print(wav_file.channels)\n",
        "  wav_file = wav_file.set_channels(1)\n",
        "\n",
        "  wav_file.export(path.join(content/gdrive/My Drive/Colab Datasets/Dataset), \"file.wav\", format=\"wav\")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!pip install pydub\\nfrom pydub import AudioSegment\\nfor file in glob.glob(\"/content/gdrive/My Drive/Colab Datasets/RAVDESS_Dataset/Actor_*//*.wav\"):\\n  wav_file = AudioSegment.from_file(file = file, format=\"wav\")\\n  #print(wav_file.channels)\\n  wav_file = wav_file.set_channels(1)\\n\\n  wav_file.export(path.join(content/gdrive/My Drive/Colab Datasets/Dataset), \"file.wav\", format=\"wav\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzwvo82kn5K4"
      },
      "source": [
        "##**Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMkeRephn-Ys",
        "outputId": "8f6c3956-4450-4a74-dfc0-a05159369777"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "s = StandardScaler()\n",
        "s.fit(x_train)\n",
        "#print(X_train)\n",
        "x_train_scaled = s.transform(x_train)\n",
        "x_test_scaled = s.transform(x_test)\n",
        "\n",
        "#Perform PCA\n",
        "\n",
        "pca = PCA()\n",
        "x_train = pca.fit_transform(x_train_scaled)\n",
        "x_test = pca.transform(x_test_scaled)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"explained_variance \",explained_variance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "explained_variance  [2.57389090e-01 8.47241531e-02 5.26199794e-02 4.81547657e-02\n",
            " 4.06468677e-02 3.24882054e-02 2.97072195e-02 2.75447289e-02\n",
            " 2.54223608e-02 2.43796607e-02 2.07316568e-02 1.85745279e-02\n",
            " 1.70296119e-02 1.64438094e-02 1.52339500e-02 1.34068550e-02\n",
            " 1.20916937e-02 1.17783281e-02 1.13203735e-02 1.09646602e-02\n",
            " 9.84151914e-03 9.64557740e-03 9.29649416e-03 8.64190589e-03\n",
            " 8.34043009e-03 7.47071412e-03 7.32986401e-03 7.03032355e-03\n",
            " 6.75405778e-03 6.25675978e-03 5.91094808e-03 5.71195334e-03\n",
            " 5.28144432e-03 5.02442493e-03 4.82750334e-03 4.48418830e-03\n",
            " 4.41535773e-03 4.26100166e-03 4.20828876e-03 3.90664412e-03\n",
            " 3.72084979e-03 3.67124312e-03 3.39986735e-03 3.35297916e-03\n",
            " 3.26704909e-03 3.12117314e-03 2.93796083e-03 2.84675284e-03\n",
            " 2.74318832e-03 2.64450261e-03 2.49915188e-03 2.40538139e-03\n",
            " 2.30968367e-03 2.21805823e-03 2.11281936e-03 2.08987721e-03\n",
            " 2.02546724e-03 2.00266328e-03 1.91190332e-03 1.84987505e-03\n",
            " 1.76765884e-03 1.71352784e-03 1.63462056e-03 1.54060951e-03\n",
            " 1.47705811e-03 1.40444097e-03 1.39342061e-03 1.38299348e-03\n",
            " 1.30966811e-03 1.29952314e-03 1.25492591e-03 1.21273830e-03\n",
            " 1.19627964e-03 1.14385729e-03 1.09075793e-03 1.03270680e-03\n",
            " 1.01894901e-03 9.59390984e-04 9.39304034e-04 9.21835644e-04\n",
            " 8.69857808e-04 8.49840844e-04 7.90281898e-04 7.79747324e-04\n",
            " 7.61165837e-04 7.37720513e-04 7.13299003e-04 6.65628874e-04\n",
            " 6.18445459e-04 6.08731492e-04 5.98493675e-04 5.92415393e-04\n",
            " 5.72559967e-04 5.39361137e-04 5.23954766e-04 4.84182930e-04\n",
            " 4.76524103e-04 4.42990444e-04 4.38911342e-04 4.27859910e-04\n",
            " 4.12342121e-04 3.95324076e-04 3.84405910e-04 3.64845572e-04\n",
            " 3.52931703e-04 3.35911193e-04 3.20217676e-04 2.98741211e-04\n",
            " 2.96936024e-04 2.86226317e-04 2.76693383e-04 2.73197686e-04\n",
            " 2.59068762e-04 2.47080872e-04 2.41536563e-04 2.39906144e-04\n",
            " 2.33950341e-04 2.13738821e-04 2.11910602e-04 2.04532582e-04\n",
            " 1.99803195e-04 1.89647273e-04 1.82065999e-04 1.73402514e-04\n",
            " 1.64058887e-04 1.56061800e-04 1.48147892e-04 1.38325139e-04\n",
            " 1.29505523e-04 1.26810599e-04 1.18458103e-04 1.12069168e-04\n",
            " 1.07297895e-04 1.03092895e-04 9.89856250e-05 9.51506986e-05\n",
            " 8.66408667e-05 8.14790200e-05 7.95913260e-05 7.77888780e-05\n",
            " 7.54493120e-05 6.99664498e-05 6.56600452e-05 5.58019359e-05\n",
            " 5.48434203e-05 5.03445415e-05 4.56373781e-05 4.46235618e-05\n",
            " 3.99897206e-05 3.79152837e-05 3.39018273e-05 3.15908811e-05\n",
            " 3.06146649e-05 2.85946818e-05 2.54590569e-05 2.31868311e-05\n",
            " 2.17635866e-05 2.03229742e-05 1.89306963e-05 1.79444342e-05\n",
            " 1.63327306e-05 1.50688989e-05 1.31707280e-05 1.22260680e-05\n",
            " 1.11620737e-05 1.05746659e-05 9.55954516e-06 8.61017237e-06\n",
            " 8.37041056e-06 7.64525510e-06 6.59100557e-06 6.06507180e-06\n",
            " 5.82268179e-06 5.62932325e-06 5.07184534e-06 4.72565142e-06\n",
            " 4.53959949e-06 3.87218244e-06 3.45349490e-06 3.25535128e-06\n",
            " 3.05443802e-06 2.72174608e-06 2.12907441e-06 1.90225602e-06\n",
            " 1.85224231e-06 1.50160032e-06 1.30407742e-06 1.13112613e-06\n",
            " 1.10334887e-06 8.64205368e-07 6.16173606e-07 5.50353307e-07\n",
            " 5.15258628e-07]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs5NUgPMwaTB"
      },
      "source": [
        "# **Classification**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwQ0A2SL2izu"
      },
      "source": [
        "scoring = {'accuracy':make_scorer(accuracy_score),\n",
        "           'precision':make_scorer(precision_score, average='micro'),\n",
        "           'recall':make_scorer(recall_score, average='micro'),\n",
        "           'f1_score':make_scorer(f1_score, average='micro')}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LX75rzw3N-3"
      },
      "source": [
        "#Initialize all the ml classifiers\n",
        "\n",
        "#Multilayer Perceptron Classifier\n",
        "m1_model=MLPClassifier(alpha=0.005, batch_size=256, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)\n",
        "\n",
        "#Support Vector Classifier\n",
        "s1_model=SVC(C=0.001, kernel='poly', gamma=0.001)\n",
        "\n",
        "#Random Forest Classifier\n",
        "r1_model=RandomForestClassifier(max_depth=7, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=40)\n",
        "\n",
        "#Decision Tree Classifier\n",
        "d1_model=DecisionTreeClassifier(criterion='entropy', max_depth=7, max_features=None, min_samples_leaf=1, min_samples_split=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "BvMzFKp-38rc",
        "outputId": "524fc911-2657-4747-b188-2cbbc971a253"
      },
      "source": [
        "'''\n",
        "#Classifier evaluation function\n",
        "\n",
        "def classifier_evaluation(X, y, folds):\n",
        "\n",
        "  X : features\n",
        "  y : target\n",
        "  folds : no. of cross-validation folds\n",
        "  \n",
        "  \n",
        "  m1 = cross_validate(m1_model, X, y, cv=folds, scoring=scoring)\n",
        "  s1 = cross_validate(s1_model, X, y, cv=folds, scoring=scoring)\n",
        "  r1 = cross_validate(r1_model, X, y, cv=folds, scoring=scoring)\n",
        "  d1 = cross_validate(d1_model, X, y, cv=folds, scoring=scoring)\n",
        "\n",
        "  #Data frame for score table\n",
        "\n",
        "  classifier_score_table = pd.DataFrame({'Multilayer Perceptron Classifier':[m1['test_accuracy'].mean(),\n",
        "                                                                             m1['test_precision'].mean(),\n",
        "                                                                             m1['test_recall'].mean(),\n",
        "                                                                             m1['test_f1_score'].mean()],\n",
        "                                         \n",
        "                                         'Support Vector Classifier':[s1['test_accuracy'].mean(),\n",
        "                                                                      s1['test_precision'].mean(),\n",
        "                                                                      s1['test_recall'].mean(),\n",
        "                                                                      s1['test_f1_score'].mean()],\n",
        "                                         \n",
        "                                         'Random Forest Classifier':[r1['test_accuracy'].mean(),\n",
        "                                                                     r1['test_precision'].mean(),\n",
        "                                                                     r1['test_recall'].mean(),\n",
        "                                                                     r1['test_f1_score'].mean()],\n",
        "                                         \n",
        "                                         'Decision Tree Classifier':[d1['test_accuracy'].mean(),\n",
        "                                                                     d1['test_precision'].mean(),\n",
        "                                                                     d1['test_recall'].mean(),\n",
        "                                                                     d1['test_f1_score'].mean()]},\n",
        "                                        \n",
        "                                          index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "\n",
        "  #Best Score Column\n",
        "  classifier_score_table['Best Score']=classifier_score_table.idxmax(axis=1)\n",
        "\n",
        "  return(classifier_score_table)\n",
        "\n",
        "classifier_evaluation(x_train, y_train, 5)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#Classifier evaluation function\\n\\ndef classifier_evaluation(X, y, folds):\\n\\n  X : features\\n  y : target\\n  folds : no. of cross-validation folds\\n  \\n  \\n  m1 = cross_validate(m1_model, X, y, cv=folds, scoring=scoring)\\n  s1 = cross_validate(s1_model, X, y, cv=folds, scoring=scoring)\\n  r1 = cross_validate(r1_model, X, y, cv=folds, scoring=scoring)\\n  d1 = cross_validate(d1_model, X, y, cv=folds, scoring=scoring)\\n\\n  #Data frame for score table\\n\\n  classifier_score_table = pd.DataFrame({'Multilayer Perceptron Classifier':[m1['test_accuracy'].mean(),\\n                                                                             m1['test_precision'].mean(),\\n                                                                             m1['test_recall'].mean(),\\n                                                                             m1['test_f1_score'].mean()],\\n                                         \\n                                         'Support Vector Classifier':[s1['test_accuracy'].mean(),\\n                                                                      s1['test_precision'].mean(),\\n                                                                      s1['test_recall'].mean(),\\n                                                                      s1['test_f1_score'].mean()],\\n                                         \\n                                         'Random Forest Classifier':[r1['test_accuracy'].mean(),\\n                                                                     r1['test_precision'].mean(),\\n                                                                     r1['test_recall'].mean(),\\n                                                                     r1['test_f1_score'].mean()],\\n                                         \\n                                         'Decision Tree Classifier':[d1['test_accuracy'].mean(),\\n                                                                     d1['test_precision'].mean(),\\n                                                                     d1['test_recall'].mean(),\\n                                                                     d1['test_f1_score'].mean()]},\\n                                        \\n                                          index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\\n\\n  #Best Score Column\\n  classifier_score_table['Best Score']=classifier_score_table.idxmax(axis=1)\\n\\n  return(classifier_score_table)\\n\\nclassifier_evaluation(x_train, y_train, 5)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU3maRrbuuNc"
      },
      "source": [
        "### **Multi Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "UgcUPkUzwb1H",
        "outputId": "87668e7f-315e-4982-eb11-9583d909adfd"
      },
      "source": [
        "\n",
        "#initialize the model\n",
        "m1=MLPClassifier(alpha=0.005, batch_size=256, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)\n",
        "\n",
        "#Train the model\n",
        "m1.fit(x_train,y_train)\n",
        "\n",
        "#Predict for the test set\n",
        "m1y_pred=m1.predict(x_test)\n",
        "\n",
        "#Calculate the accuracy of our model\n",
        "acc_m1=accuracy_score(y_true=y_test, y_pred=m1y_pred)\n",
        "#f_s1=fbeta_score(y_test, s1y_pred, beta=0.5, average='weighted')\n",
        "f1_m1=f1_score(y_true=y_test, y_pred=m1y_pred, average='weighted')\n",
        "wP_m1=precision_score(y_true=y_test, y_pred=m1y_pred, average='weighted')\n",
        "wR_m1=recall_score(y_true=y_test, y_pred=m1y_pred, average='weighted')\n",
        "'''\n",
        "#Calculate fbeta score\n",
        "f_m1=fbeta_score(y_test, m1y_pred, beta=0.5, average='micro')\n",
        "'''\n",
        "#Calculate confusion matrix\n",
        "#cm1=confusion_matrix(y_test, m1y_pred, labels=observed_emotions, average='weighted').astype(np.float32)\n",
        "\n",
        "#Print the accuracy\n",
        "print(\"Accuracy for MLP : {:.2f}%\".format(acc_m1*100))\n",
        "print(\"d1 for MLP : {:.2f}%\".format(f1_m1*100))\n",
        "print(\"wp for MLP : {:.2f}%\".format(wP_m1*100))\n",
        "print(\"wr for MLP : {:.2f}%\".format(wR_m1*100))\n",
        "'''\n",
        "#Display confusion matrix\n",
        "plt.imshow(cm1, cmap=\"binary\")\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for MLP : 67.26%\n",
            "d1 for MLP : 67.36%\n",
            "wp for MLP : 68.93%\n",
            "wr for MLP : 67.26%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#Display confusion matrix\\nplt.imshow(cm1, cmap=\"binary\")\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdz1rDkvSYB"
      },
      "source": [
        "### **Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THnMprm3G8m5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7a0d914d-9348-4b11-c599-ac779454bee9"
      },
      "source": [
        "'''\n",
        "#initialize the model\n",
        "s1=SVC(C=0.001, kernel='poly', gamma=0.001)\n",
        "\n",
        "#Train the model\n",
        "s1.fit(x_train, y_train)\n",
        "\n",
        "#Predict for the test set\n",
        "s1y_pred=s1.predict(x_test)\n",
        "\n",
        "#Calculate the accuracy of our model\n",
        "acc_s1=accuracy_score(y_true=y_test, y_pred=s1y_pred)\n",
        "\n",
        "#Calculate fbeta score\n",
        "f_s1=fbeta_score(y_test, s1y_pred, beta=0.5, average='micro')\n",
        "\n",
        "#Calculate confusion matrix\n",
        "cm2=confusion_matrix(y_test, s1y_pred, labels=observed_emotions).astype(np.float32)\n",
        "\n",
        "#Print the accuracy\n",
        "print(\"Accuracy for SVC : {:.2f}%\".format(acc_s1*100))\n",
        "\n",
        "#Display confusion matrix\n",
        "plt.imshow(cm2, cmap=\"binary\")\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#initialize the model\\ns1=SVC(C=0.001, kernel=\\'poly\\', gamma=0.001)\\n\\n#Train the model\\ns1.fit(x_train, y_train)\\n\\n#Predict for the test set\\ns1y_pred=s1.predict(x_test)\\n\\n#Calculate the accuracy of our model\\nacc_s1=accuracy_score(y_true=y_test, y_pred=s1y_pred)\\n\\n#Calculate fbeta score\\nf_s1=fbeta_score(y_test, s1y_pred, beta=0.5, average=\\'micro\\')\\n\\n#Calculate confusion matrix\\ncm2=confusion_matrix(y_test, s1y_pred, labels=observed_emotions).astype(np.float32)\\n\\n#Print the accuracy\\nprint(\"Accuracy for SVC : {:.2f}%\".format(acc_s1*100))\\n\\n#Display confusion matrix\\nplt.imshow(cm2, cmap=\"binary\")\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FhLfcKNWbUB"
      },
      "source": [
        "### **Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "W8azO42yWiG4",
        "outputId": "8e1a25c0-42ca-484d-95e6-56e48b8172f0"
      },
      "source": [
        "\n",
        "#initialize the model\n",
        "r1=RandomForestClassifier(max_depth=7, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=40)\n",
        "\n",
        "#Train the model\n",
        "r1.fit(x_train, y_train)\n",
        "\n",
        "#Predict for the test set\n",
        "r1y_pred=r1.predict(x_test)\n",
        "\n",
        "#Calculate the accuracy of our model\n",
        "\n",
        "acc_r1=accuracy_score(y_true=y_test, y_pred=r1y_pred)\n",
        "#f_s1=fbeta_score(y_test, s1y_pred, beta=0.5, average='weighted')\n",
        "f1_r1=f1_score(y_true=y_test, y_pred=r1y_pred, average='weighted')\n",
        "wP_r1=precision_score(y_true=y_test, y_pred=r1y_pred, average='weighted')\n",
        "wR_r1=recall_score(y_true=y_test, y_pred=r1y_pred, average='weighted')\n",
        "#Calculate fbeta score\n",
        "#f_r1=fbeta_score(y_test, r1y_pred, beta=0.5, average='micro')\n",
        "\n",
        "#Calculate confusion matrix\n",
        "#cm3=confusion_matrix(y_test, r1y_pred, labels=observed_emotions).astype(np.float32)\n",
        "\n",
        "#Print the accuracy\n",
        "print(\"Accuracy for MLP : {:.2f}%\".format(acc_r1*100))\n",
        "print(\"d1 for MLP : {:.2f}%\".format(f1_r1*100))\n",
        "print(\"wp for MLP : {:.2f}%\".format(wP_r1*100))\n",
        "print(\"wr for MLP : {:.2f}%\".format(wR_r1*100))\n",
        "'''\n",
        "plt.imshow(cm3, cmap=\"binary\")\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for MLP : 57.14%\n",
            "d1 for MLP : 57.12%\n",
            "wp for MLP : 57.16%\n",
            "wr for MLP : 57.14%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nplt.imshow(cm3, cmap=\"binary\")\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TMOMn1xXaID"
      },
      "source": [
        "### **Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "y01IQzhlXpn0",
        "outputId": "434a9928-c7d4-4459-d9b6-cc575be56060"
      },
      "source": [
        "\n",
        "#initialize the model\n",
        "d1=DecisionTreeClassifier(criterion='entropy', max_depth=7, max_features=None, min_samples_leaf=1, min_samples_split=2)\n",
        "\n",
        "#Train the model\n",
        "d1.fit(x_train, y_train)\n",
        "\n",
        "#Predict for the test set\n",
        "d1y_pred=d1.predict(x_test)\n",
        "\n",
        "\n",
        "acc_d1=accuracy_score(y_true=y_test, y_pred=d1y_pred)\n",
        "#f_s1=fbeta_score(y_test, s1y_pred, beta=0.5, average='weighted')\n",
        "f1_d1=f1_score(y_true=y_test, y_pred=d1y_pred, average='weighted')\n",
        "wP_d1=precision_score(y_true=y_test, y_pred=d1y_pred, average='weighted')\n",
        "wR_d1=recall_score(y_true=y_test, y_pred=d1y_pred, average='weighted')\n",
        "#Calculate fbeta score\n",
        "#f_r1=fbeta_score(y_test, r1y_pred, beta=0.5, average='micro')\n",
        "\n",
        "#Calculate confusion matrix\n",
        "#cm3=confusion_matrix(y_test, r1y_pred, labels=observed_emotions).astype(np.float32)\n",
        "\n",
        "#Print the accuracy\n",
        "print(\"Accuracy for MLP : {:.2f}%\".format(acc_d1*100))\n",
        "print(\"d1 for MLP : {:.2f}%\".format(f1_d1*100))\n",
        "print(\"wp for MLP : {:.2f}%\".format(wP_d1*100))\n",
        "print(\"wr for MLP : {:.2f}%\".format(wR_d1*100))\n",
        "\n",
        "'''\n",
        "#Display confusion matrix\n",
        "plt.imshow(cm4, cmap=\"binary\")\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for MLP : 47.62%\n",
            "d1 for MLP : 47.31%\n",
            "wp for MLP : 47.30%\n",
            "wr for MLP : 47.62%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#Display confusion matrix\\nplt.imshow(cm4, cmap=\"binary\")\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    }
  ]
}